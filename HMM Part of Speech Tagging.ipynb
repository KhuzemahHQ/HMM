{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Model (HMM) Based Part Of Speech Tagging\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this notebook, we will be making a Part of Speech (POS) Tagger using an HMM via Supervised Matrix/Parameter Initialization and Viterbi Decoding.\n",
    "\n",
    "Hidden Markov Model (HMM) is a statistical model used to describe a system that transitions between a set of hidden states over time, generating observable outcomes.\n",
    "\n",
    "In the context of Part-of-Speech (POS) tagging, HMMs are used to model the sequence of words in a sentence as a sequence of hidden states representing the POS tags. The observable outcomes are the actual words in the sentence.\n",
    "\n",
    "## Terminology\n",
    "\n",
    "__Supervised Matrix/Parameter Initialization__: involves setting the initial values of the parameters in the model, specifically the emission probabilities matrix and the transition probabilities matrix. In the case of supervised learning tasks, where the HMM is trained on annotated data (e.g., for Part-of-Speech tagging), the initialization involves estimating the initial values of these matrices based on the observed training data. This initialization is crucial as it provides the starting point for the model to learn and adjust its parameters during the training process. The matrices are typically initialized using statistical information derived from the frequency of transitions and emissions observed in the training dataset.\n",
    "\n",
    "__Viterbi Decoding__: is a dynamic programming algorithm used for finding the most likely sequence of hidden states (POS tags) given the observed sequence of words. It efficiently calculates the probability of a sequence of states by considering both emission and transition probabilities. Viterbi decoding helps identify the most probable sequence of POS tags for a given sentence based on the trained HMM parameters.\n",
    "\n",
    "![The 3 Main Questions of HMMs](hmm_questions.png)\n",
    "\n",
    "## Resources\n",
    "\n",
    "For additional details of the working of HMMs, Matrix Initializations and Viterbi Decoding, you can also consult [Chapter 8](https://web.stanford.edu/~jurafsky/slp3/8.pdf) of the SLP3 book as reference.\n",
    "\n",
    "For a more colorful tutorial, you can also refer to this guide [Hidden Markov Models - An interactive illustration](https://nipunbatra.github.io/hmm/)\n",
    "\n",
    "Another hands-on approach to Viterbi Decoding, can be found in this medium article [Intro to the Viterbi Algorithm](https://medium.com/mlearning-ai/intro-to-the-viterbi-algorithm-8f41c3f43cf3) and can be supplemented by the following slide-set  \n",
    "[HMM : Viterbi algorithm -a toy example](https://www.cis.upenn.edu/~cis2620/notes/Example-Viterbi-DNA.pdf) from the UPenn CIS 2620 course.\n",
    "\n",
    "\n",
    "For this notebook, we will stick to standard libraries i.e. `numpy`, `nltk`, `collections` and `tqdm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\khuze\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\khuze\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#Download the dataset and tagset from below:\n",
    "nltk.download('conll2000')\n",
    "nltk.download('universal_tagset')\n",
    "from nltk.corpus import conll2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS HMM Class\n",
    "The __POS_HMM__ Class contains the following methods:  \n",
    "\n",
    "1. `__init__(self, corpus)`: initializes the __POS_HMM__ and prepares it for the parameter initialization phase, contains:\n",
    "    - a corpus, which is further 85-15 split into training and test sets.\n",
    "    - a list of all the (word, tag) pairs in the training set (all the sentences are concatenated hence sequence is maintained).\n",
    "    - a tuple of unique words, tags in the training set\n",
    "    - a dictionary for mapping words and tags to its unique integer identifier.\n",
    "    - some additional variables to reduce code redundancy in latter parts such as len()\n",
    "    - Transition, Emission and Initial State Probability Matrices which are initialized to Zeros.\n",
    "    - Tag Occurance Probability matrix, which is initialized with tag probabilities i.e. \n",
    "        (count of a single tag / count of all the tags) in the training set similar to a unigram. This is used for assigning tags to new or __unknown words__ by randomly sampling from all the tag probabilities.\n",
    "2. `init_params(self)`: initializes the transition, emission and initial state probability matrices via supervised matrix/parameter learning.  \n",
    "    - a __method__ `word_given_tag(word, tag)`, which takes as input a word and a tag, and __*returns the count of all instances where the word had the assigned tag as its label / count of the tag*__ (i.e. a probability estimate of its occurence).\n",
    "    - a __method__ `next_tag_given_prev_tag(tag2, tag1)`which takes as input two tags, and __*returns the count of all instances where the tag 2 and was preceeded by tag 1 / count of the first tag*__ (also a probability estimate).\n",
    "    - code for populating the initial state probability matrix. This essentially contains the probabilities of all POS tags occuring at the start of the sentence.\n",
    "    - code to normalize each rows of the transition, emission and initial state probability (only has one row) matrices.\n",
    "3. `viterbi_decoding(self, sentence)`: returns the mostly likely POS Tags for each word/numeral/punctuation in the sentence.\n",
    "4. `evaluation(self, debug = False)`: evalutes the performance of the POS Tagger on the test set and returns the testing accuracy (as a %age)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POS_HMM:\n",
    "    def __init__(self, corpus): #DO NOT MODIFY THIS FUNCTION\n",
    "\n",
    "        #-----------------DO NOT MODIFY ANYTHING BELOW THIS LINE-----------------\n",
    "        self.corpus = corpus\n",
    "        self.train_set, self.test_set = train_test_split( self.corpus, train_size=0.85,random_state = 777)        \n",
    "        \n",
    "        # Extracting Vocabulary and Tags from our training set\n",
    "        self.all_pairs = [(word, tag) for sentence in self.train_set for word, tag in sentence] #List of tuples (word, POS tag) or a flattened list of tuples by concatenating all sentences\n",
    "        self.vocab = tuple(set(word for (word, _) in self.all_pairs))\n",
    "\n",
    "        self.all_tags = [tag for (_, tag) in self.all_pairs] #List of all POS tags in the trainset\n",
    "        self.tags = tuple(set(self.all_tags)) #List of unique POS tags\n",
    "\n",
    "        # Mapping vocab and tags to integers for indexing\n",
    "        self.vocab2index = {word: i for i, word in enumerate(self.vocab)}\n",
    "        self.tag2index = {tag: i for i, tag in enumerate(self.tags)}\n",
    "\n",
    "        self.vocab_len = len(self.vocab) #Total Number of Vocab (Unique Words)\n",
    "        self.all_tag_lengths = len(self.all_tags) #Number of tags in the trainset\n",
    "        self.tag_len = len(self.tags) #Number of unique tags\n",
    "\n",
    "        # Initialize transition and emission matrices (Default: Zeros)\n",
    "        self.transition_mat = np.zeros((self.tag_len, self.tag_len))\n",
    "        self.emission_mat = np.zeros((self.tag_len, self.vocab_len))\n",
    "        self.initial_state_prob = np.zeros(self.tag_len)\n",
    "\n",
    "        # Initialize POS Tag occurance probabilities for getting most likely POS Tags for unknown words\n",
    "        self.tag_occur_prob= {} #Dictionary of POS Tag occurance probabilities\n",
    "        all_tag_counts = Counter(self.all_tags)\n",
    "        for tag in self.tags:\n",
    "            self.tag_occur_prob[tag] = all_tag_counts[tag]/self.all_tag_lengths            \n",
    "        \n",
    "\n",
    "    def init_params(self): #Initialize transition and emission matrices via Supervised Learning (Counting Occurences of emissions and transitions observed in the data).\n",
    "        all_pairs_array = np.array(self.all_pairs)\n",
    "        tags_array = np.array(self.all_tags)\n",
    "\n",
    "        # make data structures for going through tag once and getting all probabilities of words the tag occurs with\n",
    "        # memoization of all words seen for tag\n",
    "\n",
    "        tag_dict = dict()\n",
    "        tag_counts = dict()\n",
    "        for tag in self.tags:\n",
    "            word_dict = dict()\n",
    "\n",
    "            for word in self.vocab:\n",
    "                word_dict[word] = 0 # 1 if smoothed\n",
    "\n",
    "            count_tag = 0 # self.vocab_len if smoothed?\n",
    "            for pair in all_pairs_array:\n",
    "                if pair[1] == tag:\n",
    "                    count_tag += 1\n",
    "                    word_dict[pair[0]] += 1\n",
    "            \n",
    "            tag_dict[tag] = word_dict\n",
    "            tag_counts[tag] = count_tag\n",
    "\n",
    "        def word_given_tag(word, tag): #Complete this function, skeleton and dummy variables have been provided\n",
    "            count_tag = tag_counts[tag]\n",
    "            count_w_given_tag = (tag_dict[tag])[word]\n",
    "\n",
    "            # print(\"word,count_w_given_tag,count_tag = \", word,count_w_given_tag, count_tag )\n",
    "\n",
    "            return count_w_given_tag/count_tag\n",
    "        \n",
    "        def next_tag_given_prev_tag(tag2, tag1): #Complete this function, skeleton and dummy variables have been provided\n",
    "            count_t1 = 0 # self.tag_len\n",
    "            count_t2_t1 = 0 # 1\n",
    "\n",
    "            prev = None\n",
    "            for cur_tag in tags_array:\n",
    "                if prev == tag1:\n",
    "                    count_t1 += 1\n",
    "                    if cur_tag == tag2:\n",
    "                        count_t2_t1 += 1\n",
    "                prev = cur_tag\n",
    "            \n",
    "            # Since the last word is not being checked for being tag1 in the above loop\n",
    "            if self.all_tags[-1] == tag1:\n",
    "                count_t1 += 1\n",
    "\n",
    "            return count_t2_t1/count_t1 #Number of Occurences of a tag (2nd tag) given prev tag (1st tag) / Number of Occurences of tag 1\n",
    "        \n",
    "        # Compute Transition Matrix\n",
    "        for i, t1 in enumerate(tqdm(list(self.tag2index.keys()), desc=\"Populating Transition Matrix\", mininterval = 10)):\n",
    "            for j, t2 in enumerate(list(self.tag2index.keys())):\n",
    "                self.transition_mat[i, j] = next_tag_given_prev_tag(t2, t1)\n",
    "\n",
    "        # Compute Emission Matrix\n",
    "        for i, tag in enumerate(tqdm(list(self.tag2index.keys()), desc=\"Populating Emission Matrix\", mininterval = 10)):\n",
    "            for j, word in enumerate(list(self.vocab2index.keys())):\n",
    "                self.emission_mat[i, j] = word_given_tag(word, tag)\n",
    "\n",
    "        # Compute Initial State Probability\n",
    "                \n",
    "        for i, tag in enumerate(tqdm(list(self.tag2index.keys()), desc=\"Populating Initial Probability Matrix\", mininterval = 10)):\n",
    "           self.initial_state_prob[i] = self.tag_occur_prob[tag]\n",
    "        \n",
    "        # Normalize matrices i.e. each row sums to 1\n",
    "        \n",
    "        self.transition_mat = normalize(self.transition_mat, axis=1, norm='l1')\n",
    "        self.emission_mat = normalize(self.emission_mat, axis=1, norm='l1')\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def viterbi_decoding(self, sentence): # Sentence is a list words i.e. [\"Moon\", \"Landing\", \"was\", \"Faked\"]\n",
    "        \n",
    "        pred_pos_sequence = []  # Implement the Viterbi Algorithm to predict the POS tags of the given sentence\n",
    "\n",
    "        num_states = self.tag_len\n",
    "        sent_len = len(sentence)\n",
    "\n",
    "        # Creating matrices to store memoized values\n",
    "        tag_pointers = np.zeros([sent_len, self.tag_len])\n",
    "        probabilities = np.zeros([sent_len, self.tag_len])\n",
    "        probabilities[0, :] = self.initial_state_prob\n",
    "        try:\n",
    "            current_word_ind = self.vocab2index[sentence[0]]  \n",
    "            probabilities[0, :] =  self.emission_mat[:, current_word_ind] #  * self.initial_state_prob \n",
    "        except: \n",
    "            pass       \n",
    "\n",
    "        # Bottom up Approach as it is a Dynamic Algorithm\n",
    "        for i in range(1,sent_len):\n",
    "            # Looking for word in known vocabulary and assigning an index\n",
    "            try:\n",
    "                current_word_ind = self.vocab2index[sentence[i]]\n",
    "                for s in range(num_states):\n",
    "                    max_prob = 0.0\n",
    "                    pointer = 0\n",
    "                    for s_prev in range(num_states):\n",
    "                        prob = probabilities[i-1, s_prev] * self.transition_mat[s_prev, s] * self.emission_mat[s, current_word_ind]\n",
    "                        if prob > max_prob:\n",
    "                            max_prob = prob\n",
    "                            pointer = s_prev\n",
    "                    probabilities[i, s] = max_prob\n",
    "                    tag_pointers[i, s] = pointer\n",
    "\n",
    "            # using the tag probabilities if it is a new word\n",
    "            except:\n",
    "                for s in range(num_states):\n",
    "                    max_prob = 0.0\n",
    "                    pointer = 0\n",
    "                    tag = self.tags[s]\n",
    "                    for s_prev in range(num_states):\n",
    "                        prob = probabilities[i-1, s_prev] * self.transition_mat[s_prev, s] * self.tag_occur_prob[tag]\n",
    "                        if prob > max_prob:\n",
    "                            max_prob = prob\n",
    "                            pointer = s_prev\n",
    "                    probabilities[i, s] = max_prob\n",
    "                    tag_pointers[i, s] = pointer\n",
    "\n",
    "\n",
    "        best_path = np.zeros(sent_len, dtype=int)\n",
    "        # The best path ends at the index of the last word's most probable tag\n",
    "        best_path[sent_len-1] = np.argmax(probabilities[sent_len-1, :])\n",
    "        # Going backwards trhough the stored matrices by following the pointers\n",
    "        for i in range(sent_len-2, -1, -1):\n",
    "            best_path[i] = tag_pointers[i+1, best_path[i+1]]\n",
    "        \n",
    "        # Converting indices to tags\n",
    "        for pos in best_path:\n",
    "            pred_pos_sequence.append(self.tags[pos])\n",
    "                    \n",
    "        return pred_pos_sequence\n",
    "\n",
    "    def evaluation(self, debug=False): #DO NOT MODIFY THIS FUNCTION\n",
    "        # Evaluate the model on the test set\n",
    "        correct, total = 0, 0\n",
    "        pred_pos_sequences = []\n",
    "\n",
    "        for test_sentence in self.test_set:\n",
    "            test_sentence_words, test_sentence_tags = zip(*test_sentence)\n",
    "            pred_pos_tags = self.viterbi_decoding(test_sentence_words)\n",
    "            pred_pos_sequences.extend(pred_pos_tags)\n",
    "\n",
    "            correct += sum(1 for true_tag, pred_tag in zip(test_sentence_tags, pred_pos_tags) if true_tag == pred_tag)\n",
    "            total += len(test_sentence_words)\n",
    "        \n",
    "        accuracy = correct / total if total > 0 else 1\n",
    "\n",
    "        if debug:\n",
    "            test_words, test_tags = zip(*[(word, tag) for test_sentence in self.test_set for word, tag in test_sentence])\n",
    "            print(f\"Sentence (first 20 words): {test_words[:20]}\")\n",
    "            print(f\"True POS Tags (first 20 words): {test_tags[:20]}\")\n",
    "            print(f\"Predicted POS Tags (first 20 words): {pred_pos_sequences[:20]}\")\n",
    "\n",
    "        print(f\"Test Accuracy: {accuracy * 100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Populating Transition Matrix: 100%|██████████| 12/12 [00:15<00:00,  1.31s/it]\n",
      "Populating Emission Matrix: 100%|██████████| 12/12 [00:00<00:00, 57.45it/s]\n",
      "Populating Initial Probability Matrix: 100%|██████████| 12/12 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 96.0369%\n"
     ]
    }
   ],
   "source": [
    "pos_hmm = POS_HMM(corpus = conll2000.tagged_sents(tagset='universal'))\n",
    "pos_hmm.init_params()\n",
    "pos_hmm.evaluation(debug = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
